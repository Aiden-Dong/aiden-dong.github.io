---
pin: true
title:      机器学习篇 | 机器学习常见距离汇总

date:       2020-12-29
author:     Aiden
image: 
    path : source/internal/post-bg-coffee.jpeg
categories : ['AI']
tags : ['ML']
---

对函数 `dist(.,.)`, 若它是一个"距离度量", 则需要满足一些基本性质 : 

- **非负性** : $dist(x_i, x_j) \geq 0$
- **同一性** : $dist(x_i, x_j) = 0$ 当且仅当 $x_i = x_j$
- **对称性** : $dist(x_i, x_j) = dist(x_j, x_i)$
- **直递性** : $dist(x_i, x_j) \leq dist(x_i, x_k) + dist(x_k, x_j)$

需要注意的是, 通常我们是基于某种形式的距离来定义"相似度度量",  距离越大， 相似度越小, 然而用于相似度度量的距离未必一定要满足距离度量的所有基本性质, 尤其是直递性。

对于给定样本 $x_{i} = (x_{i1}; x_{i2}; ...; x_{in})$ 与 $x_{j} = (x_{j1}; x_{j2}; ...; x_{jn})$


### 闵可夫斯基距离

闵可夫斯基距离是一种比较常见的用户相似度的计算方式

$$dist_{mk}(x_{i}, x_{j}) = (\sum_{u=1}^{n}|x_{iu} - x_{ju}|^{p})^{\frac{1}{p}}$$

#### 曼哈顿距离

对于闵可夫斯基距离, 当 `p=1` 时, 表示**曼哈顿距离** 

$$dist_{man}(x_{i}, x_{j}) = \begin{Vmatrix} x_{i} - x_{j} \end{Vmatrix}_{1} = \sum_{u=1}^{n} |x_{iu} - x_{ju}|$$

#### 欧氏距离

对于闵可夫斯基距离， 当 `p=2` 时， 表示 **欧氏距离**

$$dist_{ed}(x_{i}, x_{j}) = \begin{Vmatrix} x_{i} - x_{j} \end{Vmatrix}_{2} = (\sum_{u=1}^{n} |x_{iu} - x_{ju}|^{2})^{\frac{1}{2}}$$

#### 切比雪夫距离

对于闵可夫斯基距离, 当 $p \rightarrow \infty$ 时, 表示 **切比雪夫距离**

$$dist_{che}(x_{i}, x_{j}) = \begin{Vmatrix} x_{i} - x_{j} \end{Vmatrix}_{\infty} = \lim_{p \rightarrow \infty} (\sum_{u=1}^{n} |x_{iu} - x_{ju}|^{\infty})^{\frac{1}{\infty}} = max_{u=1}^{n}|x_{iu} - x_{ju}|$$



> 闵可夫斯基距离存在的问题

1. 将各个分量的量纲， 当做相同的权重来看待
2. 没有考虑各个分量的分布,期望，方差等可能是不同的

### 标准化欧氏距离

对于数据各维度分量的分布不一致的情况， 可以将分量都标准化到**均值**,**方差**都相等的情况

假设样本集$X$的数学期望或均值为`M`, 
标准差为 `S`. 那么 $X$ 的标准化变量$X^{*}$表示为 : 

$$X^{*} = \frac{X - M}{S}$$

**标准化后， 数学期望为0, 方差为1**

### 马氏距离

欧氏距离的一种修正, 又花了欧氏距离中各个维度尺度不一致且相关的问题。

单个数据点的马氏距离 : 

$$dist_{ma}(x_{i}) = \sqrt{(x_{i} - \mu)^{T}C^{-1}(x_{i} - \mu)}$$

数据点之间的马氏距离 : 

$$dist_{ma}(x_{i}, x_{j}) = \sqrt{(x_{i} - x_{j})^{T}C^{-1}(x_{i} - x_{j})}$$

其中, $C$ 是多维随机变量 $X$样本 的协方差矩阵, $\mu$ 为样本均值, 如果协方差矩阵是单位向量, 也就是各维度独立分布, 马氏距离就变成欧氏距离。
若协方差矩阵是对角矩阵, 马氏距离就变成标准化欧氏距离。


> 马氏距离的优缺点 


1. 它不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关
2. 由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同
3. 排除变量之间的相关性的干扰
4. 夸大了变化微小的变量的作用




### 余弦相似度

几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。

$$dist_{cos}(x_{i}, x_{j}) = \frac{x_{i} \cdot x_{j}}{|x_{i}| |x_{j}|}$$

夹角余弦取值范围为`[-1,1]`。 夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。

当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。

### 皮尔逊相关系数

相关系数是一种衡量向量相似性的方法。

而Pearson相关性系数可以看出是升级版的欧氏距离平方，因为它提供了对于变量取值范围不同的处理步骤。
因此对不同变量间的取值范围没有要求（unit free），最后得到的相关性所衡量的是趋势，而不同变量量纲上差别在计算过程中去掉了，等价于z-score标准化.

利用相关系数公式 : 

$$\rho_{XY} = \frac{Cov(X,Y)}{\sigma(X)\sigma(Y)} = \frac{E\{[X-E(X)][Y-E(Y)]\}}{\sigma(X)\sigma(Y)} = \frac{E(XY) - E(X)E(Y)}{\sigma(X)\sigma(Y)}$$

将相关系数利用到向量 $x_{i}$, $x_{j}$ 中。 
对于每个样本 $x_{i}$ 的均值我们表述为 $\mu_i = \frac{1}{n}\sum_{u=1}^{n}x_{iu}$
$x_{i}$ 的标准差可以表述为 $\sigma(x_{i}) = \frac{1}{n}\sum_{u=1}^{n}(x_{iu} - \mu_{i})$ 则有： 

$$\rho_{x_{i},x_{j}} = \frac{Cov(x_{i},x_{j})}{\sigma(x_{i}) \sigma(x_{j})} = \frac{\frac{1}{n} \sum_{u=1}^{n}(x_{iu} - \mu_{i})(x_{ju} - \mu_{j})}{\frac{1}{n}\sigma(x_{i})\sigma(x_{j})} = \frac{\sum_{u=1}^{n}(x_{iu} - \mu_{i})(x_{ju} - \mu_{j})}{\sigma(x_{i})\sigma(x_{j})}$$

> 说明 

- 当 $\rho = 0$ 时， 向量 $x_{i}$, $x_{j}$ 无相关性
- 当 $\rho > 0$ 时， 向量 $x_{i}$, $x_{j}$ 正相关
- 当 $\rho < 0$ 时， 向量 $x_{i}$, $x_{j}$ 负相关


### 杰卡德相似系数

杰明德相似系数用在集合相似性比较方面， 两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示 :


$$J(A,B) = \frac{|A\cap B|}{|A \cup B|}$$



---

> 参考内容 : 

- 周志华 - 机器学习
- 概率论与数理统计
- [机器学习总结之——各种距离汇总](https://blog.csdn.net/weixin_42715356/article/details/82845376)
- [马氏距离](https://zhuanlan.zhihu.com/p/46626607)
- [协方差矩阵](https://zhuanlan.zhihu.com/p/37609917)