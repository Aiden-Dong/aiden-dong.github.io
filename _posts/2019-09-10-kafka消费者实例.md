---
pin: true
title:      Kafka 教程 | 消费者实例

date:       2019-09-10
author:     Aiden
image: 
    path : source/internal/data-stream.jpg
categories : ['分布式']
tags : ['消息队列']
---


应用从Kafka中读取数据需要使用KafkaConsumer订阅主题，然后接收这些主题的消息。在我们深入这些API之前，先来看下几个比较重要的概念。

### Kafka消费者相关的概念

#### 1.消费者与消费组:

假设这么个场景：我们从Kafka中读取消息，并且进行检查，最后产生结果数据。
我们可以创建一个消费者实例去做这件事情，但如果生产者写入消息的速度比消费者读取的速度快怎么办呢？
这样随着时间增长，消息堆积越来越严重。对于这种场景，我们需要增加多个消费者来进行水平扩展。

**Kafka消费者是消费组的一部分，当多个消费者形成一个消费组来消费主题时，每个消费者会收到不同分区的消息。**

假设有一个T1主题，该主题有4个分区；同时我们有一个消费组G1，这个消费组只有一个消费者C1。那么消费者C1将会收到这4个分区的消息，如下所示：

![image.png]({{ site.url }}/source/nodebook/kafka_8_1.png)

如果我们增加新的消费者C2到消费组G1，那么每个消费者将会分别收到两个分区的消息，如下所示：

![image.png]({{ site.url }}/source/nodebook/kafka_8_2.png)

如果增加到4个消费者，那么每个消费者将会分别收到一个分区的消息，如下所示：

![image.png]({{ site.url }}/source/nodebook/kafka_8_3.png)

但如果我们继续增加消费者到这个消费组，剩余的消费者将会空闲，不会收到任何消息：

![image.png]({{ site.url }}/source/nodebook/kafka_8_4.png)

总而言之，我们可以通过增加消费组的消费者来进行水平扩展提升消费能力。这也是为什么建议创建主题时使用比较多的分区数，这样可以在消费负载高的情况下增加消费者来提升性能。另外，消费者的数量不应该比分区数多，因为多出来的消费者是空闲的，没有任何帮助。

Kafka一个很重要的特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息。换句话说，每个应用都可以读到全量的消息。
为了使得每个应用都能读到全量消息，应用需要有不同的消费组。对于上面的例子，假如我们新增了一个新的消费组G2，而这个消费组有两个消费者，那么会是这样的：

![image.png]({{ site.url }}/source/nodebook/kafka_8_5.png)

在这个场景中，消费组G1和消费组G2都能收到T1主题的全量消息，在逻辑意义上来说它们属于不同的应用。

> 如果应用需要读取全量消息，那么请为该应用设置一个消费组；如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者。

#### 2. 消费组与分区重平衡

可以看到，当新的消费者加入消费组，它会消费一个或多个分区，而这些分区之前是由其他消费者负责的；
另外，当消费者离开消费组（比如重启、宕机等）时，它所消费的分区会分配给其他分区。这种现象称为**重平衡（rebalance）。**
重平衡是Kafka一个很重要的性质，这个性质保证了高可用和水平扩展。
不过也需要注意到，**在重平衡期间，所有消费者都不能消费消息，因此会造成整个消费组短暂的不可用。**
而且，将分区进行重平衡也会导致原来的消费者状态过期，从而导致消费者需要重新更新状态，这段期间也会降低消费性能。后面我们会讨论如何安全的进行重平衡以及如何尽可能避免。

消费者通过定期发送心跳（hearbeat）到一个作为组协调者（group coordinator）的broker来保持在消费组内存活。这个broker不是固定的，每个消费组都可能不同。当消费者拉取消息或者提交时，便会发送心跳。

**如果消费者超过一定时间没有发送心跳，那么它的会话（session）就会过期，组协调者会认为该消费者已经宕机，然后触发重平衡。**

可以看到，从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；通常情况下，我们可以进行优雅关闭，这样消费者会发送离开的消息到组协调者，这样组协调者可以立即进行重平衡而不需要等待会话过期。

在0.10.1版本，Kafka对心跳机制进行了修改，将发送心跳与拉取消息进行分离，这样使得发送心跳的频率不受拉取的频率影响。
另外更高版本的Kafka支持配置一个消费者多长时间不拉取消息但仍然保持存活，
这个配置可以避免活锁（livelock）。活锁，是指应用没有故障但是由于某些原因不能进一步消费。

#### 3. 创建Kafka消费者

读取Kafka消息只需要创建一个kafkaConsumer，创建过程与KafkaProducer非常相像。
我们需要使用四个基本属性:`bootstrap.servers`、`key.deserializer`、`value.deserializer`和`group.id`。
其中，`bootstrap.servers`与创建KafkaProducer的含义一样；
`key.deserializer`和`value.deserializer`是用来做反序列化的，也就是将字节数组转换成对象；
`group.id`不是严格必须的，但通常都会指定，这个参数是消费者的消费组。

下面是一个代码样例：

```
Properties props = new Properties();
props.put("bootstrap.servers", "broker1:9092,broker2:9092");
props.put("group.id", "CountryCounter");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
KafkaConsumer<String, String> consumer = new KafkaConsumer<String,String>(props);
```

#### 4.订阅主题

创建完消费者后我们便可以订阅主题了，只需要通过调用subscribe()方法即可，这个方法接收一个主题列表，非常简单：

```
consumer.subscribe(Collections.singletonList("customerCountries"));
```
这个例子中只订阅了一个customerCountries主题。
另外，我们也可以使用正则表达式来匹配多个主题，而且订阅之后如果又有匹配的新主题，那么这个消费组会立即对其进行消费。正则表达式在连接Kafka与其他系统时非常有用。
比如订阅所有的测试主题：

```
consumer.subscribe("test.*");
```

#### 5. 拉取循环

消费数据的API和处理方式很简单，我们只需要循环不断拉取消息即可。
Kafka对外暴露了一个非常简洁的poll方法，其内部实现了协作、分区重平衡、心跳、数据拉取等功能，但使用时这些细节都被隐藏了，我们也不需要关注这些。下面是一个代码样例：

```
try {
   while (true) {  //1)
       ConsumerRecords<String, String> records = consumer.poll(100);  //2)
       for (ConsumerRecord<String, String> record : records)  //3)
       {
           log.debug("topic = %s, partition = %s, offset = %d,
              customer = %s, country = %s\n",
              record.topic(), record.partition(), record.offset(),
              record.key(), record.value());
           int updatedCount = 1;
           if (custCountryMap.countainsValue(record.value())) {
               updatedCount = custCountryMap.get(record.value()) + 1;
           }
           custCountryMap.put(record.value(), updatedCount)
           JSONObject json = new JSONObject(custCountryMap);
           System.out.println(json.toString(4))
       }
   }
} finally {
      consumer.close(); //4
}
```

其中，代码中标注了几点，说明如下：

1）这个例子使用无限循环消费并处理数据，这也是使用Kafka最多的一个场景，后面我们会讨论如何更好的退出循环并关闭。
2）这是上面代码中最核心的一行代码。我们不断调用poll拉取数据，如果停止拉取，那么Kafka会认为此消费者已经死亡并进行重平衡。参数值是一个超时时间，指明线程如果没有数据时等待多长时间，0表示不等待立即返回。
3）poll()方法返回记录的列表，每条记录包含key/value以及主题、分区、位移信息。
4）主动关闭可以使得Kafka立即进行重平衡而不需要等待会话过期。
另外需要提醒的是，消费者对象不是线程安全的，也就是不能够多个线程同时使用一个消费者对象；而且也不能够一个线程有多个消费者对象。简而言之，一个线程一个消费者，如果需要多个消费者那么请使用多线程来进行一一对应。


#### 6. 消费者配置

上面的例子中只设置了几个最基本的消费者参数，`bootstrap.servers`，`group.id`，`key.deserializer`和`value.deserializer`，其他的参数可以看Kafka文档。
虽然我们很多情况下只是使用默认设置就行，但了解一些比较重要的参数还是很有帮助的。

**fetch.min.bytes**

这个参数允许消费者指定从broker读取消息时最小的数据量。
当消费者从broker读取消息时，如果数据量小于这个阈值，broker会等待直到有足够的数据，然后才返回给消费者。
对于写入量不高的主题来说，这个参数可以减少broker和消费者的压力，因为减少了往返的时间。而对于有大量消费者的主题来说，则可以明显减轻broker压力。

**fetch.max.wait.ms**

上面的fetch.min.bytes参数指定了消费者读取的最小数据量，而这个参数则指定了消费者读取时最长等待时间，从而避免长时间阻塞。这个参数默认为500ms。

**max.partition.fetch.bytes**

这个参数指定了每个分区返回的最多字节数，默认为1M。也就是说，KafkaConsumer.poll()返回记录列表时，每个分区的记录字节数最多为1M。
如果一个主题有20个分区，同时有5个消费者，那么每个消费者需要4M的空间来处理消息。实际情况中，我们需要设置更多的空间，这样当存在消费者宕机时，其他消费者可以承担更多的分区。

> 需要注意的是，max.partition.fetch.bytes必须要比broker能够接收的最大的消息（由max.message.size设置）大，否则会导致消费者消费不了消息。另外，在上面的样例可以看到，我们通常循环调用poll方法来读取消息，如果max.partition.fetch.bytes设置过大，那么消费者需要更长的时间来处理，可能会导致没有及时poll而会话过期。对于这种情况，要么减小max.partition.fetch.bytes，要么加长会话时间。


**session.timeout.ms**

这个参数设置消费者会话过期时间，默认为3秒。也就是说，如果消费者在这段时间内没有发送心跳，那么broker将会认为会话过期而进行分区重平衡。
这个参数与heartbeat.interval.ms有关，heartbeat.interval.ms控制KafkaConsumer的poll()方法多长时间发送一次心跳，这个值需要比session.timeout.ms小，一般为1/3，也就是1秒。
更小的session.timeout.ms可以让Kafka快速发现故障进行重平衡，但也加大了误判的概率（比如消费者可能只是处理消息慢了而不是宕机）。

**auto.offset.reset**

这个参数指定了当消费者第一次读取分区或者上一次的位置太老（比如消费者下线时间太久）时的行为，可以取值为latest（从最新的消息开始消费）或者earliest（从最老的消息开始消费）。

**enable.auto.commit**

这个参数指定了消费者是否自动提交消费位移，默认为true。如果需要减少重复消费或者数据丢失，你可以设置为false。如果为true，你可能需要关注自动提交的时间间隔，该间隔由auto.commit.interval.ms设置。

**partition.assignment.strategy**

我们已经知道当消费组存在多个消费者时，主题的分区需要按照一定策略分配给消费者。这个策略由**PartitionAssignor类**决定，默认有两种策略：

范围（Range）：对于每个主题，每个消费者负责一定的连续范围分区。假如消费者C1和消费者C2订阅了两个主题，这两个主题都有3个分区，那么使用这个策略会导致消费者C1负责每个主题的分区0和分区1（下标基于0开始），消费者C2负责分区2。可以看到，如果消费者数量不能整除分区数，那么第一个消费者会多出几个分区（由主题数决定）。
轮询（RoundRobin）：对于所有订阅的主题分区，按顺序一一的分配给消费者。用上面的例子来说，消费者C1负责第一个主题的分区0、分区2，以及第二个主题的分区1；其他分区则由消费者C2负责。可以看到，这种策略更加均衡，所有消费者之间的分区数的差值最多为1。

`partition.assignment.strategy`设置了分配策略，默认为org.apache.kafka.clients.consumer.RangeAssignor（使用范围策略），你可以设置为`org.apache.kafka.clients.consumer.RoundRobinAssignor`(使用轮询策略），或者自己实现一个分配策略然后将partition.assignment.strategy指向该实现类。

**client.id**

这个参数可以为任意值，用来指明消息从哪个客户端发出，一般会在打印日志、衡量指标、分配配额时使用。

**max.poll.records:**

这个参数控制一个poll()调用返回的记录数，这个可以用来控制应用在拉取循环中的处理数据量。

**receive.buffer.bytes、send.buffer.bytes**

这两个参数控制读写数据时的TCP缓冲区，设置为-1则使用系统的默认值。如果消费者与broker在不同的数据中心，可以一定程度加大缓冲区，因为数据中心间一般的延迟都比较大。


#### 7.提交（commit）与位移（offset）

当我们调用`poll()`时，该方法会返回我们没有消费的消息。
当消息从broker返回消费者时，broker并不跟踪这些消息是否被消费者接收到；
Kafka让消费者自身来管理消费的位移，并向消费者提供更新位移的接口，这种更新位移方式称为**提交（commit）**。

在正常情况下，消费者会发送分区的提交信息到Kafka，Kafka进行记录。当消费者宕机或者新消费者加入时，Kafka会进行重平衡，这会导致消费者负责之前并不属于它的分区。
重平衡完成后，消费者会重新获取分区的位移，下面来看下两种有意思的情况。

假如一个消费者在重平衡前后都负责某个分区，如果提交位移比之前实际处理的消息位移要小，那么会导致消息重复消费，如下所示：

![image.png]({{ site.url }}/source/nodebook/kafka_8_6.png)

假如在重平衡前某个消费者拉取分区消息，在进行消息处理前提交了位移，但还没完成处理宕机了，然后Kafka进行重平衡，新的消费者负责此分区并读取提交位移，此时会“丢失”消息，如下所示：

![image.png]({{ site.url }}/source/nodebook/kafka_8_7.png)

因此，提交位移的方式会对应用有比较大的影响，下面来看下不同的提交方式。


##### 自动提交

这种方式让消费者来管理位移，应用本身不需要显式操作。
当我们将`enable.auto.commit`设置为true，那么消费者会在poll方法调用后每隔5秒（由auto.commit.interval.ms指定）提交一次位移。
和很多其他操作一样，自动提交也是由poll()方法来驱动的；
在调用poll()时，消费者判断是否到达提交时间，如果是则提交上一次poll返回的最大位移。

需要注意到，这种方式可能会导致消息重复消费。假如，某个消费者poll消息后，应用正在处理消息，在3秒后Kafka进行了重平衡，那么由于没有更新位移导致重平衡后这部分消息重复消费。

##### 提交当前位移

为了减少消息重复消费或者避免消息丢失，很多应用选择自己主动提交位移。设置auto.commit.offset为false，那么应用需要自己通过调用**commitSync()**来主动提交位移，该方法会提交poll返回的最后位移。

为了避免消息丢失，我们应当在完成业务逻辑后才提交位移。而如果在处理消息时发生了重平衡，那么只有当前poll的消息会重复消费。下面是一个自动提交的代码样例：

```
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(100);¬
    for (ConsumerRecord<String, String> record : records)
    {
        System.out.printf("topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n", record.topic(), record.partition(), record.offset(), record.key(), record.value());
    }
    
    try {
        consumer.commitSync();
    } catch (CommitFailedException e) {
        log.error("commit failed", e)
    }
}
```

上面代码poll消息，并进行简单的打印（在实际中有更多的处理），最后完成处理后进行了位移提交。


到目前为止，我们已经创建了一个发送消息到Kafka集群的生产者。
现在让我们创建一个消费者来消费Kafka集群的消息。 
`KafkaConsumer` API用于消费来自Kafka集群的消息。 

#### KafkaConsumer类的构造函数定义如下:

```
public KafkaConsumer(java.util.Map<java.lang.String,java.lang.Object> configs)
```

configs - 返回消费者配置的Map。

KafkaConsumer类具有下表中列出的以下重要方法:

方法 | 说明
--- | ---
public java.util.Set< TopicPar- tition> assignment() | 获取由用户当前分配的分区集。
public string subscription() | 订阅给定的主题列表以获取动态签名的分区。
public void sub-scribe(java.util.List< java.lang.String> topics，ConsumerRe-balanceListener listener) | 订阅给定的主题列表以获取动态签名的分区。
public void unsubscribe() | 从给定的分区列表中取消订阅主题。
public void sub-scribe(java.util.List< java.lang.String> topics) | 订阅给定的主题列表以获取动态签名的分区。 如果给定的主题列表为空，则将其视为与unsubscribe()相同。
public void sub-scribe(java.util.regex.Pattern pattern，ConsumerRebalanceLis-tener listener) | 参数模式以正则表达式的格式引用预订模式，而侦听器参数从预订模式获取通知。
public void as-sign(java.util.List< TopicPartion> partitions) | 向客户手动分配分区列表。
poll() | 使用预订/分配API之一获取指定的主题或分区的数据。 如果在轮询数据之前未预订主题，这将返回错误。
public void commitSync() | 提交对主题和分区的所有子编制列表的最后一次poll()返回的提交偏移量。 相同的操作应用于commitAsyn()。
public void seek(TopicPartition partition，long offset) | 获取消费者将在下一个poll()方法中使用的当前偏移值。
public void resume() | 恢复暂停的分区。
public void wakeup() | 唤醒消费者。

#### ConsumerRecord API:

ConsumerRecord API用于从Kafka集群接收记录。 此API由主题名称，分区号(从中接收记录)和指向Kafka分区中的记录的偏移量组成。
ConsumerRecord 类用于创建具有特定主题名称，分区计数和< key，value>的消费者记录。 对。 它有以下签名。

```
public ConsumerRecord(string topic,int partition, long offset,K key, V value)
```
- 主题 - 从Kafka集群接收的使用者记录的主题名称。
- 分区 - 主题的分区。
- 键 - 记录的键，如果没有键存在null将被返回。
- 值 - 记录内容。

ConsumerRecords API充当ConsumerRecord的容器。 此API用于保存特定主题的每个分区的ConsumerRecord列表。 它的构造器定义如下。

```
public ConsumerRecords(java.util.Map<TopicPartition,java.util.List
<Consumer-Record>K,V>>> records)
```

- TopicPartition - 返回特定主题的分区地图。
- 记录 - ConsumerRecord的返回列表。

ConsumerRecords类定义了以下方法：

方法 | 说明
--- | ---
public int count() | 所有主题的记录数。
public Set partitions() | 在此记录集中具有数据的分区集(如果没有返回数据，则该集为空)。
public Iterator iterator() | 迭代器使您可以循环访问集合，获取或重新移动元素。
public List records() | 获取给定分区的记录列表。

> 配置设置 :

Consumer客户端API主配置设置的配置设置如下所示 :

配置 | 说明
--- | ---
group.id | 将单个消费者分配给组。
enable.auto.commit | 如果值为true，则为偏移启用自动落实，否则不提交。
auto.commit.interval.ms | 返回更新的消耗偏移量写入ZooKeeper的频率。
session.timeout.ms | 表示Kafka在放弃和继续消费消息之前等待ZooKeeper响应请求(读取或写入)多少毫秒。

#### SimpleConsumer应用程序：

添加更多进程/线程将导致Kafka重新平衡。 如果任何消费者或代理无法向ZooKeeper发送心跳，则可以通过Kafka集群重新配置。 
在此重新平衡期间，Kafka将分配可用分区到可用线程，可能将分区移动到另一个进程。


```
import java.util.Properties;
import java.util.Arrays;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.ConsumerRecord;

public class SimpleConsumer {
   public static void main(String[] args) throws Exception {
      if(args.length == 0){
         System.out.println("Enter topic name");
         return;
      }
      //Kafka consumer configuration settings
      String topicName = args[0].toString();
      Properties props = new Properties();
      
      props.put("bootstrap.servers", "localhost:9092");
      props.put("group.id", "test");
      props.put("enable.auto.commit", "true");
      props.put("auto.commit.interval.ms", "1000");
      props.put("session.timeout.ms", "30000");
      props.put("key.deserializer", 
         "org.apache.kafka.common.serializa-tion.StringDeserializer");
      props.put("value.deserializer", 
         "org.apache.kafka.common.serializa-tion.StringDeserializer");
      KafkaConsumer<String, String> consumer = new KafkaConsumer
         <String, String>(props);
      
      //Kafka Consumer subscribes list of topics here.
      consumer.subscribe(Arrays.asList(topicName))
      
      //print the topic name
      System.out.println("Subscribed to topic " &plus; topicName);
      int i = 0;
      
      while (true) {
         ConsumerRecords<String, String> records = con-sumer.poll(100);
         for (ConsumerRecord<String, String> record : records)
         
         // print the offset,key and value for the consumer records.
         System.out.printf("offset = %d, key = %s, value = %s\n", 
            record.offset(), record.key(), record.value());
      }
   }
}
```
生产者应用程序步骤在此保持不变。 首先，启动你的ZooKeeper和Kafka代理。 然后使用名为 SimpleCon-sumer.java 的Java类创建一个 SimpleConsumer 应用程序，并键入以下代码。
