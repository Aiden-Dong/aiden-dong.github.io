---
pin: true
title:      Linux 教程 | Linux 平台IO技术
date:       2020-06-17
author:     Aiden
image: 
    path : source/internal/post-bg-unix-linux.jpg
categories : ['底层']
tags : ['linux']
---

### 用户空间以及内核空间概念

我们知道现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。

操心系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核，保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。

针对linux操作系统而言，将最高的1G字节（从虚拟地址`0xC0000000`到`0xFFFFFFFF`），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址`0x00000000`到`0xBFFFFFFF`），供各个进程使用，称为用户空间。每个进程可以通过系统调用进入内核，因此，Linux内核由系统内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有4G字节的虚拟空间。

![image.png]({{ site.url }}/source/nodebook/linux_io_1_1.png)

有了用户空间和内核空间，整个linux内部结构可以分为三部分，从最底层到最上层依次是：**硬件-->内核空间-->用户空间**。

![image.png]({{ site.url }}/source/nodebook/linux_io_1_2.png)

内核空间中存放的是内核代码和数据，而进程的用户空间中存放的是用户程序的代码和数据。不管是内核空间还是用户空间，它们都处于虚拟空间中。

我们都知道，为了OS的安全性等的考虑，**进程是无法直接操作I/O设备的，其必须通过系统调用请求内核来协助完成I/O动作，而内核会为每个I/O设备维护一个buffer。**

![image.png]({{ site.url }}/source/nodebook/linux_io_1_3.png)

整个请求过程为：用户进程发起请求，内核接受到请求后，从I/O设备中获取数据到buffer中，再将buffer中的数据copy到用户进程的地址空间，该用户进程获取到数据后再响应客户端。

在整个请求过程中，数据输入至buffer需要时间，而从buffer复制数据至进程也需要时间。

> 根据在这两段时间内等待方式的不同，I/O动作可以分为以下五种模式：

- 阻塞I/O (Blocking I/O)
- 非阻塞I/O (Non-Blocking I/O)
- I/O复用（I/O Multiplexing)
- 信号驱动的I/O (Signal Driven I/O)
- 异步I/O (Asynchrnous I/O) 

有关这部分的内容详情可以阅读 APUE 与 UNP 两部书。

---

### IO模型

#### 阻塞I/O (Blocking I/O)

在linux中，默认情况下所有的文件描述符都是blocking，一个典型的读操作流程大概是这样：

![image.png]({{ site.url }}/source/nodebook/linux_io_1_4.png)

当用户进程调用了`read`这个系统调用，内核就开始了IO的第一个阶段：等待数据准备。很多时候数据在一开始还没有到达，这个时候内核就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。

当内核一直等到数据准备好了，它就会将数据从内核中拷贝到用户内存，然后内核返回结果，用户进程才解除block的状态，重新运行起来。

所以，blocking IO的特点就是在IO执行的两个阶段都被block了。


#### 非阻塞I/O(Non-Blocking I/O)

linux下，在打开文件描述符是可以设置打开方式为`O_NONBLOCK`, 使其变为non-blocking。当对一个non-blocking 的文件描述符执行读操作时，流程是这个样子：

![image.png]({{ site.url }}/source/nodebook/linux_io_1_5.png)

当用户进程调用recvfrom时，系统不会阻塞用户进程，而是立刻返回一个`EWOULDBLOCK`错误, 从用户进程角度讲，并不需要等待，而是马上就得到了一个结果。
用户进程判断标志是`EWOULDBLOCK`时，就知道数据还没准备好，于是它就可以去做其他的事了，于是它可以再次发送recvfrom，一旦内核中的数据准备好了。并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。

当一个应用程序在一个循环里对一个非阻塞调用recvfrom，我们称为轮询。应用程序不断轮询内核，看看是否已经准备好了某些操作。这通常是浪费CPU时间，但这种模式偶尔会遇到。

#### I/O复用（I/O Multiplexing)

有些地方也称这种IO方式为event driven IO。

我们都知道，`select`/`epoll`的好处就在于单个process就可以同时处理多个网络连接的IO。

它的基本原理就是`select`/`epoll`这个function会不断的轮询所负责的所有`socket`，当某个socket有数据到达了，就通知用户进程。它的流程如图：

![image.png]({{ site.url }}/source/nodebook/linux_io_1_6.png)

当用户进程调用了`select`，那么整个进程会被block，而同时，内核会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从内核拷贝到用户进程。

这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。

因为这里需要使用两个system call (`select` 和 `recvfrom`)，而blocking IO只调用了一个system call (`recvfrom`)。

但是，用select的优势在于它可以同时处理多个connection。（**如果处理的连接数不是很高的话，使用`select/epoll`的web server不一定比使用`multi-threading + blocking IO`的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。**）

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为`non-blocking`，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。


有关多路复用的实现原理大家可以阅读我之前写的文章  : [socket 服务器编程里的那些事](https://aiden-dong.github.io/2019/09/26/%E6%9D%82%E8%B0%88%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0/)


#### 信号驱动的I/O (Signal Driven I/O)

信号驱动IO在编程中会接触的到， 它是将socket的事件映射到 SIGIO 信号上， 使用信号处理的一种方式。

针对一个套接字使用信号驱动式I/O，要求进程执行以下3个步骤：

```
// 建立SIGIO信号处理函数
signal(SIGIO, sig_io);
// 设置该套接字的属主进程，通常使用fcntl的F_SETOWN命令设置
fcntl(sockfd, F_SETOWN, getpid());
// 开启该套接字的信号驱动式I/O，通常通过使用fcntl的F_SETFL命令打开O_ASYNC标志
const int on = 1;
ioctl(sockfd,  O_ASYNC,  &on);
```

![image.png]({{ site.url }}/source/nodebook/linux_io_1_7.png)

很明显可以看出用户进程不是阻塞的。

首先用户进程建立SIGIO信号处理程序，并通过系统调用sigaction执行一个信号处理函数，这时用户进程便可以做其他的事了，一旦数据准备好，系统便为该进程生成一个SIGIO信号，去通知它数据已经准备好了，于是用户进程便调用recvfrom把数据从内核拷贝出来，并返回结果。


#### 异步I/O

一般来说，这些函数通过告诉内核启动操作并在整个操作（包括内核的数据到缓冲区的副本）完成时通知我们。这个模型和前面的信号驱动I/O模型的主要区别是，在信号驱动的I/O中，内核告诉我们何时可以启动I/O操作，但是异步I/O时，内核告诉我们何时I/O操作完成。

![image.png]({{ site.url }}/source/nodebook/linux_io_1_8.png)

当用户进程向内核发起某个操作后，会立刻得到返回，并把所有的任务都交给内核去完成（包括将数据从内核拷贝到用户自己的缓冲区），内核完成之后，只需返回一个信号告诉用户进程已经完成就可以了。



#### 5中I/O模型的对比


![image.png]({{ site.url }}/source/nodebook/linux_io_1_9.png)

结果表明：前四个模型之间的主要区别是第一阶段，四个模型的第二阶段是一样的：过程受阻在调用recvfrom当数据从内核拷贝到用户缓冲区。然而，异步I/O处理两个阶段，与前四个不同。

从同步、异步，以及阻塞、非阻塞两个维度来划分来看：

![image.png]({{ site.url }}/source/nodebook/linux_io_1_10.png)

---

### 零拷贝

CPU不执行拷贝数据从一个存储区域到另一个存储区域的任务，这通常用于在网络上传输文件时节省CPU周期和内存带宽。

#### 缓存 IO

缓存IO 又被称作标准IO，大多数文件系统的默认IO 操作都是缓存 IO。在 Linux 的缓存 IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（page cache）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

缓存 IO 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。

#### 零拷贝技术分类

零拷贝技术的发展很多样化, 现有的零拷贝技术种类也非常多, 而当前并没有一个适合于所有场景的零拷贝技术的出现. 

对于 Linux 来说，现存的零拷贝技术也比较多，这些零拷贝技术大部分存在于不同的Linux内核版本，有些旧的技术在不同的Linux内核版本间得到了很大的发展或者已经渐渐被新的技术所代替。本文针对这些零拷贝技术所适用的不同场景对它们进行了划分。

概括起来，Linux 中的零拷贝技术主要有下面这几种：


##### 直接 I/O

对于这种数据传输方式来说，应用程序可以直接访问硬件存储, 操作系统内核只是辅助数据传输：这类零拷贝技术针对的是操作系统内核并不需要对数据进行直接处理的情况，数据可以在应用程序地址空间的缓冲区和磁盘之间直接进行传输. 完全不需要 Linux 操作系统内核提供的页缓存的支持。

要在块设备中执行直接 I/O，进程必须在打开文件的时候设置对文件的访问模式为 `O_DIRECT`，这样就等于告诉操作系统进程在接下来使用 `read()` 或者 `write()` 系统调用去读写文件的时候使用的是直接 I/O 方式，所传输的数据均不经过操作系统内核缓存空间。

使用直接 I/O 读写数据必须要注意缓冲区对齐（ buffer alignment ）以及缓冲区的大小的问题，即对应 read() 以及 write() 系统调用的第二个和第三个参数。这里边说的对齐指的是文件系统块大小的对齐，缓冲区的大小也必须是该块大小的整数倍。


##### mmap, sendfile, splice

在数据传输的过程中，避免数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间进行拷贝。有的时候，应用程序在数据进行传输的过程中不需要对数据进行访问，那么，将数据从 Linux 的页缓存拷贝到用户进程的缓冲区中就可以完全避免，传输的数据在页缓存中就可以得到处理。在某些特殊的情况下，这种零拷贝技术可以获得较好的性能。Linux 中提供类似的系统调用主要有 `mmap()`, `sendfile()` 以及 `splice()`。


##### 写时复制

对数据在 Linux 的页缓存和用户进程的缓冲区之间的传输过程进行优化。该零拷贝技术侧重于灵活地处理数据在用户进程的缓冲区和操作系统的页缓存之间的拷贝操作。这种方法延续了传统的通信方式，但是更加灵活。在Linux 中，该方法主要利用了写时复制技术。

##### 总结

前两类方法的目的主要是为了避免应用程序地址空间和操作系统内核地址空间这两者之间的缓冲区拷贝操作。

这两类零拷贝技术通常适用在某些特殊的情况下，比如要传送的数据不需要经过操作系统内核的处理或者不需要经过应用程序的处理。

第三类方法则继承了传统的应用程序地址空间和操作系统内核地址空间之间数据传输的概念，进而针对数据传输本身进行优化。我们知道，硬件和软件之间的数据传输可以通过使用 DMA 来进行，DMA 进行数据传输的过程中几乎不需要CPU参与，这样就可以把 CPU 解放出来去做更多其他的事情，但是当数据需要在用户地址空间的缓冲区和 Linux 操作系统内核的页缓存之间进行传输的时候，并没有类似DMA 这种工具可以使用，CPU 需要全程参与到这种数据拷贝操作中，所以这第三类方法的目的是可以有效地改善数据在用户地址空间和操作系统内核地址空间之间传递的效率。

![image.png]({{ site.url }}/source/nodebook/linux_io_1_11.png)

当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据read系统调用提供的buf地址，将内核缓冲区的内容拷贝到buf所指定的用户空间缓冲区中去。

如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠DMA来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。
接下来，write系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后socket再把内核缓冲区的内容发送到网卡上。

从上图中可以看出，共产生了四次数据拷贝，即使使用了DMA来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。


在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性。


> 让数据传输不需要经过user space，使用mmap

我们减少拷贝次数的一种方法是调用mmap()来代替read调用：

```
buf = mmap(diskfd,len);
write(socketfd, buf, len);
```

应用程序调用 mmap()，磁盘上的数据会通过 DMA 被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用 write(),操作系统直接将内核缓冲区的内容拷贝到 socket缓冲区中，这一切都发生在内核态，最后， socket缓冲区再把数据发到网卡去。

![image.png]({{ site.url }}/source/nodebook/linux_io_1_12.png)


使用 `mmap` 替代 read 很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用 `mmap` 是有代价的。当你使用 `mmap` 时，你可能会遇到一些隐藏的陷阱。例如，当你的程序 `map` 了一个文件, 但是当这个文件被另一个进程截断(`truncate`)时, write系统调用会因为访问非法地址而被 `SIGBUS` 信号终止。 `SIGBUS` 信号默认会杀死你的进程并产生一个 coredump,如果你的服务器这样被中止了，那会产生一笔损失

通常我们使用以下解决方案避免这种问题：

1. 为SIGBUS信号建立信号处理程序 当遇到 SIGBUS信号时，信号处理程序简单地返回， write 系统调用在被中断之前会返回已经写入的字节数，并且 errno 会被设置成success,但是这是一种糟糕的处理办法，因为你并没有解决问题的实质核心。

2. 使用文件租借锁 通常我们使用这种方法，在文件描述符上使用租借锁，我们为文件向内核申请一个租借锁，当其它进程想要截断这个文件时，内核会向我们发送一个实时的 RT_SIGNAL_LEASE信号，告诉我们内核正在破坏你加持在文件上的读写锁。这样在程序访问非法内存并且被 SIGBUS杀死之前，你的 write系统调用会被中断。 write会返回已经写入的字节数，并且置 errno为success。 我们应该在 mmap文件之前加锁，并且在操作完文件后解锁：

```
if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) {
    perror("kernel lease set signal");
    return -1;
}

/* l_type can be F_RDLCK F_WRLCK  加锁*/

/* l_type can be  F_UNLCK 解锁*/

if(fcntl(diskfd, F_SETLEASE, l_type)){
    perror("kernel lease set type");
    return -1;
}
```